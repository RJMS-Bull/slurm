<!--#include virtual="header.txt"-->

<h1> Co-Scheduled Jobs (Job Packs or Heterogeneous Resources)</h1>
<a name="Overview"></a>
<h2>Overview</h2>
<p>The purpose of this guide is to assist Slurm users and administrators in
Co-Scheduling Jobs. The document is divided into the following sections:</p>
<ul>
<li><a href="#Overview">Overview</a></li>
<li><a href="#Section1">Definition of Terms</a></li>
<li><a href="#Section2">Specification Language and Command Line Syntax</a></li>
<li><a href="#Section3">Resource Allocation</a></li>
<li><a href="#Section4">Limitations</a></li>
</ul>


<!-- The sections do not have to be in this order. as we add no ones we need to
reorganize based on what maked sense for the end user -->


<p>Co-Scheduling of jobs is also called job_packs and sometimes refered to as
Heterogeneous Resources. It is scheduling of multiple job allocations such that
they run in parallel to run a common application. The co-scheduled jobs can all
participate in the same MPI-Comm-World.
</p>

<p>
It addresses the need to have nodes with different characteristics as part of
the same step. For example
<ul>
<li>
A node with lots of memory for the serial startup/wrapup phase.
</li>
<li>
Lots of nodes with GPU for the parallel phase.
</li>
<li>
Nodes with Fast I/O to store the results.
</li>
<li>
And these nodes run different executables that are part of the same
MPI_Comm_World
</li>
</ul>
</p>
<p>
This job could be defined as;
</p>
<p>
srun -N 1 -Cbig ./controller : -N 1000 -gres=gpu ./worker : -N 10 -pIO ./saver
</p>
<p>
Where the srun command accepts a colon separated list of Job Descriptions.
A Job Description is essentially the full set of options available for a
normal srun
</p>





<p>
</p>

<a name="Section1"></a>
<h2>Definition of Terms</h2>

<p>
<b>Job Pack</b> (long form: Heterogeneous Job Pack) - A collection of jobs that
may have different resource specifications and other options, managed for
certain purposes (e.g., resource allocation, step launch) as a single entity.
</p>
<p>
<b>Pack Member Job</b> (short form: Pack Member) Any individual job in a
Job Pack.
</p>
<p>
<b>Pack Leader Job</b> (short form: Pack Leader) - A Pack Member Job used to
manage and reference the Job Pack as a single entity. Each Job Pack contains
only one Pack Leader Job.
</p>
<p>
<b>Job Pack Description</b> - A complete set of salloc, sbatch or standalone
srun command line elements that define all of the Pack Member Jobs in a Job Pack.
</p>
<p>
<b>Job Description</b> (long form: Pack Member Job Description) - A set of
salloc, sbatch or standalone srun command line elements that define an
individual Pack Member Job in a Job Pack. Adjacent Job Descriptions within a
Job Pack Description are separated by a colon.
</p>
<p>
<b>Pack Group</b> (long form: Job Pack Group) - An integer value used to
identify an individual Pack Member Job in a Job Pack. Pack Groups are assigned
based on the order of Job Descriptions in a Job Pack Description, starting at
zero for the first Job Description. The job identified by Pack Group 0 is the
Pack Leader Job.
</p>
<p>
Step Description (long form: Job Pack Step Description) A set of
step-launch-only srun command line elements that define a step and identify
each Pack Member Job for which the step is to be launched, using the
corresponding Pack Group. Adjacent Step Descriptions within a complete
step-launch-only srun command are separated by a colon.
</p>

<a name="Section2"></a>
<h2>Specification Language and Command Line Syntax</h2>

<p>
For Job Packs, the syntax of the salloc, sbatch and srun commands is expanded
to support multiple jobs and steps.
</p>
<h3>New salloc command syntax</h3>
salloc [options 0] [command 0 [command args 0]] [...] : [options n]
<h3>New sbatch command syntax</h3>
sbatch [options 0] script 0 [script args 0] [...] : [options n]
<h3>New srun command syntax</h3>
srun [options 0] executable 0 [executable args 0] [...] : [options n] executable n [executable args n]
<p>
Each set of colon-separated command line elements on a salloc, sbatch or
standalone srun command line is referred to as a job description. A single job
is created from each job description. For standalone srun commands, a single
step is created for each job. For step-launch-only srun commands, each set of
colon-separated elements is referred to as a step description. One or more
steps is created from each step description, depending on the value of the
--pack-group option.
</p>
<h4>New option for srun command</h4>
--pack-group=expr
<p>
Identify each job in a Job Pack for which a step is to be created. Applies only
to srun commands issued inside a salloc allocation or sbatch script. expr is a
set of integers corresponding to one or more options indexes on the salloc or
sbatch command line.
</p>
Examples:
<br />
--pack-group=2
<br />
--pack-group=[0,4]
<br />
--pack-group=[1,3-5]
<br />
The default value is --pack-group=0.





<a name="Section3"></a>
<h2>Resource Allocation</h2>

<h3>Job Submission</h3>
<p>
The salloc, sbatch, and srun clients effectively split the command line using
free standing colons. The set of option between the colons representing job pack
members. The first member is the pack leader.
</p>

<p>
Each member job description is submitted serially, from right to left on the
command line. The member jobs are submitted with a new dependency option,
-dpack. These jobs are marked pending. The pack leader is submitted with the
dependency option packleader. This option has a list of the job-id of each
member as a colon separated list.
</p>

<p>
The pack leader is scheduled in a normal fashion. When it becomes the highest
priority job in the queue, the scheduler attempts to allocate resources for
each member. This allocation attempt is not a test, but a run-now attempt.
(This leads to subtle side effects explain later.) If a subsequent member of
the job pack can't be allocated, all prior members are deallocated. All members
are pending.
</p>


<h3>Job State and Job Completion</h3>

<p>
Pack members will generally have the same state as the pack leader.
</p>
<p>
Pack members will complete when the pack leader completes. If tasks are still
running on the resources of the pack members when the pack leader completes
they will be terminated.
</p>

<h3>Backfill Scheduler</h3>


<p>
The Backfill Scheduler still works, providing lower priority jobs meet the time
limit constraints of the leader. A lower priority normal job will backfill and
run before a higher priority job pack and a job pack will run before a higher
priority normal job.
</p>

<h3>Job Preemption</h3>

<p>
Members of a job_pack cannot be preempted.
</p>

<p>
The pack leader can preempt legacy jobs, but only the resource requirements of
the leader will be used to find lower priority. (When the pack leader's request
is evaluated, the members have already been allocated.)
</p>

<p>
Pack members cannot preempt other jobs. This is because, when a job pack is to
be allocated, each member is allocated. When any member (or the leader) can't
be allocated, the previous members are deallocated. If such a deallocated job
had preempt a normal job, that normal job should still be running.
</p>

<p>
For example a job pack has three members with jobid=501,502,503. We evaluate
the requirements of 501 and it can be satisfied by preempting job 255. When
502 is evaluated, and its requirements can't be met, 501 is deallocated.
However, 255 was preempted 501 was allocated, so it is either cancelled,
or re-queued.
</p>


<h3>Side Effects of Pack Members Being Separate Jobs</h3>
<h4>Never Run Test</h4>

<p>
The fact that each resource requirement is specified as a separate job means
that it is hard to determine if a job can ever run. Tests have been implemented
to prevent the cluster from stalling, but they are weak in that a job that could
run on an idle cluster may be diagnosed as never being able to run.
</p>

<h5>Pack members require too many nodes</h5>

<p>
In a cluster of 6 nodes, <b>srun -N5 --exclusive : -N2 --exclusive : -N2 --exclusive</b> gets the following results.
</p>
<pre>
       JOBID PARTITION    NAME      USER   ST   TIME  NODES NODELIST(REASON)
         403    all     group.sh   slurm   PD   0:00    2   (Dependency)
         404    all     group.sh   slurm   PD   0:00    2   (Dependency)
         405    all     grpldr.sh  slurm   PD   0:00    5   (Resources)
</pre>

<p>
When cons-res does the 'can 405 ever run test', it believes 403 and 404 will
eventually terminate and let 405 run.
</p>

<h5>Pack Member Never Runs</h5>

<p>
On a system with 8 nodes, the following job pack will not run.
</p>

<p>
<b>srun -N2 --exclusive : -N5 --exclusive : -N5 --exclusive</b>
</p>

<p>
This condition is not detected by the will-never-run test for the pack leader
as it is only executed when the pack leader is scheduled. And a pack leader will
never be scheduled until all of its members have been allocated.
</p>

<p>
In this case, the pack leader eventually becomes the highest priority job in
the system, but it can't be scheduled because the members can't be allocated.
The system is hung, with all nodes idle and the pack leader queued with a
pending reason of none.
</p>

<p>
The existing pack leader will-never-run test can't be used because is imbedded
in pick_best_nodes with selects candidate nodes base on the allocation request.
</p>

<p>
A new test for members has been constructed in which when a member can't be
allocated the cluster is tested to see if all nodes are idle. If all nodes are
idle, then the leader and the pack jobs are killed. This too is a weak test
that might kill some job packs that theoretically could run, but at least it
doesn't allow the cluster to hang.
</p>

<h5>Never run side effect</h5>

<p>
Both never run tests may not report anything to the sbatch client. This is
because the sbatch program itself has terminated; and the script is never
launched so the output file is never created. Entries are put in the log,
but they won't be reported directly to the user.
</p>

<h4>Failed Pack Leader Anomalies</h4>

<h5>Members Remain in Pending State</h5>

<p>
When the pack leader can't be allocated (bad node name, too many nodes, ...)
the members remain pending until the next time the scheduler runs.
</p>

<p>
The members are submitted first. They get their allocations and go pending.
When the leader arrives, if it can't be scheduled, the allocate functions
return errors to the client (srun, salloc). The controller doesn't have a
chance to intervene. This condition will exist until an internal timeout expires.
</p>

<h5>Pack leader fails with Run Time Exhausted during step creation</h5>

<p>
When a member is exclusively allocated (either--exclusive or Shared=EXCLUSIVE) on a
partition, and the member is in the pack-group of more than one step
<br />(srun --pack-group=0 ... : --pack-group=0 ...) <br />
then step creation fails.
</p>

<p>
During task assignment for the first step, exclusive allocation means all cpus
are marked assigned in cons-res internal tables. When a subsequent step on
on that srun attempts task assignment it finds no cpus, so cons-res backs out
and tries later. Eventually the pack_leader times out.
</p>

<p>
When this condition occures, the job is terminated with the message
<br />
<b>JPCK: nodes  busy, assume job_pack with exclusive allocation</b>
</p>


<a name="Section4"></a>
<h2>Limitations</h2>

<h4>PrologFlags=Alloc</h4>

<p>
This slurm.conf option does not work in the current release. This limitation
will be fixed in a future release.
</p>

<p>
When test whether a job pack can run, the pack members are actually allocated
so the prolog script is excuted. If any member cannot be allocated, the
resources are deallocated, but that script will have executed, possibly taking
actions that can't be reversed.
</p>

<p>
A work around is the execute tasks that usually would be put in a prolog as a
srun on the entire allocation.
</p>





<p class="footer"><a href="#top">top</a></p>

<p style="text-align:center;">Last modified 19 October 2015</p>

<!--#include virtual="footer.txt"-->
